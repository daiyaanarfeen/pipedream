apiVersion: v1
kind: Pod
metadata:
    name: pipedream-profiler
spec:
    nodeSelector:
        nodetype: phortx

    # Restarting the head node automatically is not currently supported.
    # If the head node goes down, `ray up` must be run again.
    restartPolicy: Never

    # This volume allocates shared memory for Ray to use for its plasma
    # object store. If you do not provide this, Ray will fall back to
    # /tmp which cause slowdowns if is not a shared memory volume.
    volumes:
    - name: dshm
      emptyDir:
          medium: Memory
    - name: nfs-pvc
      persistentVolumeClaim:
          claimName: test-claim

    hostNetwork: true
    hostIPC: true
    containers:
    - name: ray-node
      imagePullPolicy: IfNotPresent
      # You are free (and encouraged) to use your own container image,
      # but it should have the following installed:
      #   - rsync (used for `ray rsync` commands and file mounts)
      #   - screen (used for `ray attach`)
      #   - kubectl (used by the autoscaler to manage worker pods)
      image: docker.pdl.cmu.edu/pipedream-image:latest
      # Do not change this command - it keeps the pod alive until it is
      # explicitly killed.
      command: ["/bin/bash", "-c", "--"]
      args: ["trap : TERM INT; sleep infinity & wait;"]
      ports:
          - containerPort: 6379 # Redis port.
          - containerPort: 6380 # Redis port.
          - containerPort: 6381 # Redis port.
          - containerPort: 12345 # Ray internal communication.
          - containerPort: 12346 # Ray internal communication.

      # This volume allocates shared memory for Ray to use for its plasma
      # object store. If you do not provide this, Ray will fall back to
      # /tmp which cause slowdowns if is not a shared memory volume.
      volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /mnt
            name: nfs-pvc
      resources:
          limits:
              nvidia.com/gpu: 8
      env:
          # This is used in the head_start_ray_commands below so that
          # Ray can spawn the correct number of processes. Omitting this
          # may lead to degraded performance.
          - name: MY_CPU_REQUEST
            valueFrom:
                resourceFieldRef:
                    resource: requests.cpu
